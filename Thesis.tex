% article.tex, a sample LaTeX file.
% Run LaTeX on this file twice for proper section numbers.
% A '%' causes LaTeX to ignore remaining text on the line

% Use the following line for draft mode (double spaced, single column)
%\documentclass[preprint,pre,floats,aps,amsmath,amssymb]{revtex4}

% Use the following line for journal mode (single spaced, double column)
\documentclass[preprint,pre,floats,aps,amsmath,amssymb,12pt]{revtex4}
\usepackage{graphicx}
\usepackage{bm}

\begin{document}

\title{Twitter Sentiment Analysis and Stock Price and Keyword Correlation}
\author{Linhao Zhang}
\affiliation{Department of Computer Science, The University of Texas at Austin}
\date{\today}

\begin{abstract}

Though uninteresting individually, Twitter messages, or tweets, can provide an accurate reflection of public sentiment on when taken in aggregation. In this paper, we apply sentiment analysis and machine learning principles to accomplish two tasks. We first look for a correlation between twitter sentiment and stock prices and volatility. Secondly, we determine which words in tweets correlate to changes in stock prices by doing a post analysis of price change and tweets. We accomplish this task by mining tweets using their search API and subsequently processing them for analysis. For the task of determining sentiment, we primarily examine the effectiveness of three machine learning techniques: Naive Bayes classification, Maximum Entropy classification, and Support Vector Machines. We discover that $TODO$ give the highest consistent accuracy through cross validation. We then find that there is $TODO$ correlation between stock prices and tweet sentiment. Next, we improve on the keyword search approach by reverse correlating stock prices to individual words in tweets. Lastly, we discuss various challenges posed by looking at twitter for performing stock predictions. 

\end{abstract}

\maketitle

\section{Introduction}
\label{sec:intro}




\section{Related Work}
\label{sec:rel}
The proliferation of online documents and user generated texts has led to a recent growth in research in the area of sentiment analysis and its relationship to financial markets. A broad overview of some of the machine learning techniques used in sentiment classification is provided in \cite{Pang} There, they investigate methods in determining whether movie reviews are positive or negative and examine some challenges in sentiment mining. They provide an overview of Naive Bayes, Maximum Entropy, and Support Vector Machine classification techniques in the movie review domain. 

\cite{Pak} discusses the use of Twitter specifically as a corpus for sentiment analysis. They discuss the methods of tweet gathering and processing. The authors use specific emoticons to form a training set for sentiment classification, a technique that greatly reduces manual tweet tagging. Their training set was split into positive and negative samples based on happy and sad emoticons. This paper borrows from this idea. Additionally, they analyze a few accuracy improvement methods. Similarly, \cite{Bifet} presents a discussion of streaming Tweet mining and sentiment extraction, while furthering the discussion to include opinion mining. 

\cite{Bollen} presented one of the first indications that there may be a correlation between Twitter sentiment and the stock market. In their work, a sentiment score is correlated with the DJIA and then fed into a neural network to predict market movements. The authors use a mood tracking tool named OpinionFinder to measure mood in 6 dimensions (Calm, Alert, Sure, Vital, Kind, and Happy). Then, they correlate the mood time series with DJIA closing values by using a Self Organizing Fuzzy Neural Network. Using their techniques, they measured an improvement on DJIA prediction accuracy. After publication, this paper launched much of the current research in the relationship between twitter and market sentiments. 

A few other techniques in the area have been proposed. \citep{Brown} provides a method in which tweets are screened for stock symbols, then sentiment is gathered using Naïve Bayes over an existing dataset. Their research continues and is still in the data collection phase. \cite{Shangkun} combines sentiment gathered from news with technical features of stock data. Sentiment is analyzed using SentiWordNet 3.0 \cite{SWN}, a lexical resource for opinion mining. Instead of Twitter, they scrape data from Engadget. Multiple kernel learning is run to find the best coefficients for the different factors in their system for stock prediction. The authors found that training a model based on more than technical stock indicators improved stock prediction performance. 

\cite{Mittal} achieves a 75\% accurate prediction mechanism using Self Organizing Fuzzy Neural Networks on Twitter and DJIA feeds. In their research, they created a custom questionnaire with words to analyze tweets for their sentiment. Their work is similar to \cite{Bollen}, with a few minor modifications.

On a side note, \cite{Hurwitz} discusses some common problems involved in many of the techniques presented above. These include, but are not limited to: insufficient data, inaccurate measures of performance, and inappropriate scaling. 

The techniques proposed by these papers provide an interesting overview of sentiment analysis and how it can relate to the stock market. However, the results seem varied and may depend heavily on the accuracy of a twitter sentiment classifier. Additionally, the market and keywords used are not detailed in most of these papers, which may play a big role in determining whether or not the system is effective in stock prediction. In this work, we build off some of these previous works to build a sentiment analyzer. However, in addition to correlating stock price, we look for correlations in particular words in tweets that indicate positive and negative market movements after-the-fact. 

\section{Proposed Model}
\label{sec:model}

\includegraphics[scale=.75]{graphic1.png}\\
Figure 1. Proposed research model

\section{Corpus Collection}
\label{sec:mining}
To train a sentiment analzyer and obtain data, we needed a system to collect tweets. Therefore, we first worked to collect a corpus of tweets that would serve as training data for our sentiment analyzer. 

It was decided that we would categorize tweets as "positive" or "negative." At first, we were unsure of whether or not to include a "neutral" sentiment label, but after [5] reported a significant decrease in sentiment accuracy when a "neutral" category was included, we decided to keep our analyzer to just "positive" or "negative." 

At first, we considered manually tagging tweets with a "positive" or "negative" label. [http://www.sananalytics.com] does sentiment analysis based off a database of 5513 hand-classified tweets. A search for other tweet corpuses returned no results, as Twitter had recently modified its terms of service to disallow public hosting of old tweets. We decided that tagging tweets manually would work, but there were better methods in which to form a training set. In the end, we decided to emulate a technique proposed in [Pak] to collect a corpus with positive and negative sentiments without any manual effort for classification. To do this, we employed use of Twitters Search API. 

\subsection{Twitter Search/Streaming API}
For tweet collection, Twitter provides a rather robust API. There are two possible ways to gather Tweets: the Streaming API or the Search API. The Streaming API allows users to obtain real-time access to tweets from an input query. The user first requests a connection to a stream of tweets from the server. Then, the server opens a streaming connection and tweets are streamed in as they occur, to the user. 

However, there are a few limitations of the Streaming API. First, language is not specifiable, resulting in a stream that contains Tweets of all languages, including a few non-Latin based alphabets. Additionally, at the free level, the streamed Tweets are only a small fraction of the actual Tweet body (gardenhose vs. firehose). Initial testing with the streaming API resulted in a polluted training set as it proved difficult to obtain a pure dataset of Tweets.

Because of these issues, we decided to go with the Twitter Search API instead. The Search API is a REST API which allows users to request specific queries of recent tweets. The Search API allows more fine tuning of queries, including filtering based on language, region, and time. There is a rate limit associated with the query, but we handle it in the code. For our purposes, the rate limit has not been an issue. To actually fetch tweets, we continuously send queries to the Search API, with a small delay to account for the rate limit. The query is constructed by stringing separate keywords together with an "OR" in between. Though this is also not a fully complete result, it returns a well filtered set of tweets that is useful for our sentiment analyzer. 

The request returns a list of JSON objects that contain the tweets and their metadata. This includes a variety of information, including username, time, location, retweets, and more. For our purposes, we mainly focus on the time and tweet text. 

Both of these APIs are require the user have an API key for authentication. Once authenticated, we were able to easily access the API through a python library called Twython- a simple wrapper for the Twitter API.

\subsection{Training Set Collection}
Using Twitter's search API, we formed two separate datasets (collections of Tweets) for training: "positive" and "negative." Each dataset was formed programmatically and based on positive and negative queries on emoticons and keywords:

\begin{itemize}
\item Positive sentiment query: ":) OR :-) OR =) OR :D OR <3 OR like OR love"
\item Negative sentiment query: ":( OR =( OR hate OR dislike"
\end{itemize}

The reasoning behind these keywords is straightforward. The tweets that contained these keywords or emoticons were most likely to be of that corresponding sentiment. This results in a training set of "positive" and "negative" tweets that was nearly as good as manual tagging. Another advantage of programmatically mining tweets with prior sentiment is that we were able to mine many, many more. In all, we gathered over 500,000 tweets of each sentiment. Unfortunately, I did not have the computational power to train over all the tweets, but the sample size was definitely increased from manually annotating tweets with a sentiment label. 

\subsection{MongoDB Storage}

To save our training tweet data, we used MongoDB. MongoDB is a document based database in which data is stored as JSON-like objects. Naturally, this works well with the tweet objects returned from the API. Each tweet is simply stored as a record in a collection in the database. Not all the data is stored, though- only relevant information, including tweet text and date. Querying the MongoDB database is simple as well- one iterator simply crawls through an entire collection of tweets during training and classification tasks.  

\section{Tweet Text Processing}
\label{sec:txt}

The text of each tweet contains many extraneous words that do not contribute to its sentiment. Many tweets include URLS, tags to other users, or symbols that have no meaning. To accurately obtain a tweet's sentiment, we first need to filter the noise from its original state. To do this, we incorporate a variety of techniques. An example of common tweet characters and formats can be seen in Table 1. Before processing, many of the tweets characters and words merely add noise to our sentiment analysis. \\

\includegraphics[scale=.75]{tweet_table.png}\\
Table 1. Tweet examples

\subsection{Tokenization}

The first step involves splitting the text by spaces, forming a list of individual words per text. This is also called a bag of words. We will later use each word in the tweet as features to train our classifier. 

\subsection{Removing Stopwords}

Next, we remove stopwords from the bag of words. Python's Natural Language Toolkit library contains a stopword dictionary. To remove the stopwords from each text, we simply check each word in the bag of words against the dictionary. If a word is a stopword, we filter it out. The list of stopwords contains articles, some prepositions, and other words that add no sentiment value (able, also, or, etc.)

\subsection{Twitter Symbols}

Many tweets contain extra symbols such as "@" or \"\#," as well as URLs. The word immediately following an "@" symbol is always a username, which we filter out entirely, as they add no value to the text. Words following \"\#" are kept, for they may contain information about the tweet, especially for categorization. URLs are filtered out entirely, as they add no sentiment meaning to the text. To accomplish all of this, we use a regex that matches for these symbols.  Additionally, any non-word symbols in the bag of words are filtered out as well.

\section{Training the Classifiers}
\label{sec:train}

Once we gather a large tweet corpus with "positive" and "negative" sentiment, we can build and train a classifier. We examine three types of classifiers: Naive Bayes, Maximum Entropy, and Support Vector Machine. For each, we extract the same features from the Tweets to classify on.

\subsection{Feature Extraction}
Features were chosen based on simplicity and results from previous works in the area. The process of choosing which features to use was not straightforward. Eventually, we settled upon building our featureset purely on whether or not certain N-grams exist within the tweet. 

\subsubsection{Unigrams}
A unigram is simply an N-gram of size one, or a single word. For each unique word in a tweet, a unigram feature is created for the classifier. For example, if a positive tweet contains the word "market," a feature for classification would be whether or not a tweet contains the word "market." Since the feature came from a positive tweet, the classifier would be more likely to classify other tweets containing the word "market" as positive. 

\subsubsection{N-grams}
We extract bigrams and trigrams from our tweets as features to train our classifier. Bigrams are pairs of consecutive words, which adds to the accuracy of the classifier by distinguishing words such as "don't like" or "not happy." Similarly, Trigrams are triplets of consecutive words. These too add to the information coverage of the classifier. In our N-grams, gap skipping is not allowed, and the words must follow each other. When we add bigrams and trigrams, we increase the features in our featureset by $n-1$ for bigrams and $n-2$ for trigrams, where $n$ is the number of individual words amongst all of the tweets in our corpora. Though this greatly increases training and classification time, [$CITE$]'s work as well as our own experimental classifiers show us that bigrams and trigrams increase the accuracy of the classifier. 

\subsubsection{External Lexicon}
We feed in features from an external lexicon called SentiStrength $CITE$, which is a list of words that are predefined with a sentiment, either positive or negative. Their data is applied to "short texts," which is perfect for short things. The inclusion of the SentiStrength Lexicon allows for a broader coverage of words that may be missed by merely collecting from Tweets. We spent time looking into more freely available lexicons, and discovered that there are far too many to be able to feasibly measure the effectiveness of each of them. Therefore, we decided that lexicon analysis was out of scope for this paper and are only including SentiStrength in our classifier featureset. 

\subsubsection{Part of Speech Tagging}
Other works attempt to use Part of Speech Tagging to varying degrees of success. Part of Speech Tagging involves marking up words in a tweet with a corresponding part of speech, as well as its context. With that information, certain features can theoretically be eliminated or more heavily weighted $CITE$'s work stated that they did not find any significant classification improvement using this technique. Therefore, while we did tinker with a system that allowed part of speech tagging on tweets, we decided that it was ultimately not worth the additional effort, and left it out of our design. 

\subsubsection{Neutral Labels}
We considered applying a third label, "neutral," as a possible classification for our tweets. However, we ran into a few challenges and found evidence against using this label. First, it proved difficult to gather "neutral" tweets. We considered scraping objective news tweet streams, such as the Wall Street Journal or New York Times, but guessed that the words gathered would largely be domain specific and add much to the classifier. Additionally, [$cite$]'s work showed that adding a third "neutral" label to a classifier in fact decreased its accuracy and was not beneficial for their system. In the spirit of simplicity, we decided to follow suit and not use a "neutral" label in our classifier.

Of course, there are downsides to simply categorizing every tweet as "positive" or "negative." In future work, we definitely will consider adding other dimensions of sentiment beyond positive and negative. 

\subsection{Classifiers}
Accurate classification is still an interesting problem in machine learning and data mining. Many times, we want to build a classifier with a set of training data and labels. In our case, we want to construct a classifier that is trained on our "positive" and "negative" labeled tweet corpus. From this, the classifier will be able to label future tweets as either "positive" or "negative," based on the tweet's attributes or features. Here, we examine three common classifiers used for text classification: Naive Bayes, Maximum Entropy, and Support Vector Machines. In each of these classifiers, previous work has demonstrated a certain level of effectiveness in text classification. 

In the following examples, $c$ will represent the class label, which in our case is either "positive" or "negative," an $f_{i}$ represents a feature in the featureset $F$. 


\subsubsection{Naive Bayes}
A Naive Bayes classifier is probabilistic classifier based on Bayes Rule, and the simplest form of a Bayesian network. The classifier is simple to implement and widely used in many applications. The classifier operates on an underlying "naive" assumption of independence about each feature in its featureset.

The classifier is an application of Bayes Rule:
\[ P(c|F) = \frac{P(F|c) P(c)}{P(F)} \]

In our context, the we are looking for a class, $c$, so we find the most probable class given features, $F$. We can treat the denominator, $P(F)$ as a constant, for it does not depend on $c$ and the values are provided. Therefore, we must focus on solving the numerator. To do this, we need to determine the value of $P(F|c)$. Here is where the independence assumption comes in. We assume that the features are independent of each other, therefore:

\[ P(f_{1}, f_{2} \ldots f_{n}|c_{j}) = \prod_{i} P(f_{i} | c_{j}) \]

From this, we can classify a tweet with a label $c^{*}$ with a maximum a posterior decision rule taking the most probable label from all labels $C$. 

\[\ c^{*} = \arg\max_{c_{j} \in C} P(c_{j})
\prod_{i} P(f_{i} | c_{j}) \]	

The Naive Bayes classifier is extremely simple, and its conditional independence assumptions are not realistic in the real world. However, applications of Naive Bayes classifiers have performed well, better than initially imagined. Past work from [$cite Zhang wiki$] discusses the surprising performance of the Naive Bayes classifier and proposes that the distribution of dependencies among all attributes over a class leads to an optimal classification. 

However, we also looked at a few other, more sophisticated classifiers as alternatives, and later evaluate the performance of each. 

\subsubsection{Maximum Entropy}

A second classifier that we used is the Maximum Entropy classifier, or MaxEnt. Though less popular than the Naive Bayes classifier, the MaxEnt classifier removes independence assumptions between features, even with a one to one featureset mapping. This advantage leads the classifier to perform better than Naive Bayes in certain situations. 
In a maximum entropy model, each feature corresponds to a constraint on the model. The classifier computes the maximum entropy value from all the models that satisfy the constraints of the features. The features of a MaxEnt classifier are usually restricted to binary functions that characterize a specific feature its class.

The MaxEnt estimation of $P(c | f)$ has the parametric form:
			
\[ P(c | f) = \frac{1}{Z(f)}\exp \left((\displaystyle\sum\limits_{i} \lambda_{i,c} F_{i,c}(f,c)\right)\]

In this equation, $Z(d)$ is a normalization function, and $F_{i,c}$ is a binary function that takes as input a feature and a class label. It is defined as:
\[ F_{i,c}(f,c') =  \left\lbrace
\begin{array}{lr}
1,\ n(f) > 0 $ and $  c' = c \\
0 \ $otherwise$	 
\end{array}
\right.\]

This binary function is trigged when a certain feature (unigram, bigram, etc.) exists and the sentiment is hypothesized in a certain way. Again, this makes no assumptions about the conditional relationships between each feature. 

The $\lambda$ is a vector of weight parameters for the featureset. These parameter variables are estimated by a technique called Generalized Iterative Scaling, or GIS. GIS iteratively updates the values for the parameters that satisfy the tweets feature constraints while continuing to maximize the entropy of the model. Eventually, the iterations converge the model to an optimal maximum entry for the probability distribution. In our context, we iterate a maximum of 10 times to train our classifier. Practically, for this, the time it takes to train increases greatly compared to a Naive Bayes classifier.

\subsubsection{Support Vector Machine}

The third classifier we use in our analysis is the Support Vector Machine, or SVM. In previous works, SVMs have been shown to be very effective for text categorization. The SVM is a classifier that attempts to find a separation between a linearly separable set of data, with as wide of a gap as possible between them, called a margin. Unlike the Naive Bayes and MaxEnt classifiers, the SVM is a margin classifier, rather than probabilistic. With an input training set, the SVM finds the hyperplane such that each point is correctly classified and the hyperplane is maximally far from the closest points. The name "support vector" comes from points on the margin between the hyerplane and the nearest data points, which are called support vectors. 
The SVM looks for a parameter vector $/alpha$ that, again, maximizes the distance between the hyperplane and every training point. In essence, it is an optimization problem:

\[ \text{Minimize} \ \frac{1}{2} \alpha \cdot \alpha \]
\[\text{s.t.} \ c_{i}(\alpha\cdot f_{i} + b) \geq 1 \ \ \  \forall1,\ldots, n \]

Here, $c_{i}$ is the class label, $\{1, -1\}$ for positive and negative, that corresponds to the training feature vector $f_{i}$. One property to note is that the distance of any of the training points from the hyperplane margin will be no less than $\frac{1}{||\alpha||}$.
The solution of the problem can be written as:

\[ w = \displaystyle\sum\limits_{j} \alpha_{j} c_{j} d_{j}, \ \alpha_{j} \geq 0 \]

Once the SVM is built, classification of new tweets simply involves determining which side of the hyperplane that they fall on. 
In our case, there are only two classes, which simplifies the SVM to a linear classifier. However, the SVM is flexible enough in non-linear situations using the kernel trick, which transforms their input into a higher dimensional space.

\section{Classifier Evaluation}
\label{sec:eval}

To continue to our ultimate goal of applying Twitter sentiment to the financial market, we trained and tested each of our classifiers on a subset of our tweet corpus. Though we gathered a few hundred thousand tweets, we were only able to train on a seven thousand, due to memory constraints. In the future, work on memory optimizations and more powerful or distributed machines should allow us to train on more data. 

For each of the classifiers, performed a 5-fold cross validation and found the average accuracy. N-fold cross validation means splitting the training data into N sets. One of those sets is left out as a test set to measure accuracy, and the N-1 other sets are used to train the classifier. This is repeated N total times, one for each separate data partition. Afterwards, the accuracies were averaged and reported. 

Additionally, we measured precision and recall values for each of the labels, "positive" and "negative". Precision here is measured as the number of true correct results over the all the positive results, or: 

\[\text{Precision} = \frac{tp}{tp + fp}\]

where $tp$ means true positive (correct result) and $fp$ means false positive (undesired result).

Recall is the true positive rate of the classifier, and is measured as:

\[\text{Recall} = \frac{tp}{tp + fn}\]

where $fn$ means false negative (missing result).


\subsection{Naive Bayes}

With the Naive Bayes classifier, 5-fold cross validation yielded an average accuracy of $accuracy$. These results will be discussed more when I have them.

\subsection{Maximum Entropy}


With the Maximum Entropy classifier, 5-fold cross validation yielded an average accuracy of $accuracy$. These results will be discussed more when I have them.


\subsection{Support Vector Machine}


With the Support Vector Machine, 5-fold cross validation yielded an average accuracy of $accuracy$. These results will be discussed more when I have them.

A summary of our results: \\

\includegraphics[scale=.8]{acc_table.png} \\
Table 2. Classifier results summary

\section{Market Correlation}
\label{sec:corr}

After training and saving our classifier, we were able to begin looking at correlation between tweet sentiment and stock market prices. To do so, we first had to build a system to collect stock data, as well as tweets during market hours. For the scope of the project, we decided to look for intraday correlation with a lag period of $k$ minutes. Additionally, we focused our scope only on technology stocks. Therefore, we gathered minute by minute data on a few of the most popular technology ETFs. 


\subsection{Stock Mining}

ETFs, or exchange traded funds, generally track an index. This way, they are good representations of the state of an entire market. We used Yahoo's finance API to gather data on 10 ETFs throughout the trading day, which was 8:30-3:00 PM Central time. Yahoo's data was lagged by 15 minutes, so we had to accommodate for that while computing our correlation lag values. We store each gathered price and its timestamp in a MongoDB collection, for later processing.

\subsection{Tweet Mining}

In a similar manner to gathering our tweet corpus from above, we used Twitter's search API to gather tweets about technology from 8:30-3:00 PM Central time, same as market hours. The keywords we used were: 
$list keywords$. 
These keywords were chosen arbitrarily, and in future work, would best be integrated with the post-correlated keywords part of the project. 
Again, we stored the gathered tweets in a MongoDB collection. After gathering the tweets, we classified each of them using our saved classifier and stored the results in another database collection. 

\subsection{Realized Variance}

\subsection{Methods}

\subsection{Correlation Results}
We will have correlation results soon.



\section{Significant Keyword Correlation}
\label{sec:keyword}
After gathering a significant amount of tweets and price information, we decided to additionally look for correlation between significant words and stock prices. Because of our difficulty coming up with the best related keywords for twitter, we realized there was a parallel problem we could solve with our data. This time, instead of looking for a correlation between stock prices and tweet sentiments, we decided to work backwards and examine which specific words from tweets were associated with changes in stock prices. 

\subsection{Methods}
We need no additional data to perform this task. We have already gathered our technology related tweets, and have our stock prices. Again, we are looking to see which specific words seem to lead to a change in stock prices. To do this, we:
\begin{enumerate}
\item Associate each tweet and each stock price with a timestamp, measured to the closest minute.
\item Choose a set of lag values, $k$, to iterate through. (With the +15 minute Yahoo lag compensation)
\item Create a dictionary mapping individual words to a score for every minute $m$. 
\item For each minute $m$, compute the difference in current stock price, $d$, with the price after a certain lag. 
\item Every word per tweet at minute $m$ adds $d$ to their score. 
\item Afterwards, return the words with highest and lowest total scores.
\end{enumerate}

We are only interested in the extremes, as the highest score indicates words which are associated with positive stock price change, while the lowest scores are associated with negative stock price change. At the end of iterating through each minute in a market day for a number of lag values, we are left with a list of words which are most strongly correlated with stock price change, either positive or negative. 

\subsection{Results}
Coming soon!

\section{Discussion}
\label{sec:disc}

Coming soon!

\section{Challenges}
\label{sec:chal}

Throughout the course of the project, we ran across a number of challenges. The scope of the challenges varied greatly, and we were able to overcome some, but not all. 

The first challenge we discovered was a matter of tweet searching. Though we had a system set up to process tweets, it was difficult to choose the right keywords to search on. In the end, our method was a bit arbitrary, for we had no stronger alternative. We tried using different stock symbols to collect our tweets, but Though the keywords chosen matter less and less with a larger amount of tweets, they still play a large role in determining how much correlation there is between tweet sentiment and stock market prices. 

Another inherent challenge with twitter sentiment is differentiating between slang or jargon and real English. For example, many people substitute "your" with "ur," or "hello" with "yo." Though we are able to understand slang when we read it, it is harder for a computer to understand. Therefore, many important, sentiment-contributing words are potentially lost. This is a challenging problem to solve, and perhaps more work will be done on it in the future. 

Additionally, we ran into the challenge of computation time and power. Because we didn't have immediate access to large amounts of storage, we trained many of our classifiers on a limited number of tweets. Though the system is able to train on much more data, we didn't have the time or computation power at hand. This challenge can be addressed by taking the project to a larger scale, over a few years. This way, there would be more data to train on and more time to do the training. 

\section{Future Work}
\label{sec:future}
There are many areas in which this work could be expanded on in the future. With a longer period of time and more resources, there is much potential in the area. 

One idea for future improvement is to gather data for a longer period of time, and with different granularities. One of the shortcomings of this study was the lack of time needed (years) for best data collection. If possible, we would want to collect data over the course of a few years, both from Twitter and the stock market. This way, we could also alter the granularity of the data. Instead of looking at intraday trends between Twitter and the stock market, we would be able to look at larger scales of correlation, between days or weeks. Intuitively, we feel that decreasing the granularity of the data would provide for a stronger correlation, as the sentiment would be more accurate. Also, sentiment, a macro measurement, may take a while to manifest itself in the stock market- more than just in a few minutes or hours. We would be able to have more tweets for both training and analyzing, also, from a longer period of time. 

Another natural expansion of the project would be to actually make stock predictions and trade money based on twitter sentiment. To do this, w would follow some existing examples of using a Neural Network for prediction. However, we would not want to solely rely on twitter sentiment- we would want to incorporate other technical indicators. This is quite a bit more research and knowledge, and was out of scope of this project. Nevertheless, it would be an interesting area of future study, as the options and opportunities in the area of stock price prediction are endless. 

Lastly, another area of future work would be to better incorporate the significant words determined by this paper into the keywords used to look for technology related tweets. Naturally, by first discovering which individual words seem to correlate to changes in stock prices, and then building a dataset from those words, we might be able to see a stronger correlation to certain stocks. Though this is more narrow of a project and would be less generalizable, it would be more practical for real money trading. The words may only relate to a specific stock or set of stocks, but that would be okay. 

\begin{acknowledgments}

This project would not have been possible without the guidance of Dr. Inderjit Dhillon, the advisor for this study. Additionally, thanks go out to Dr. Aziz Adnan for his mentorship, and Dr. William Press for assisting on the Undergraduate Thesis Comittee. Lastly, thanks to my team in 1 Semester Startup- Ashton Grimball, George Su, and Patrick Day, for introducing me to this area of research. 

\end{acknowledgments}




\begin{thebibliography}{99}

\bibitem{Pang}B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up? Sentiment Classification using Machine Learning Techniques. {\it Proceedings of EMNLP} Publication. 79-86: Cornell, 2002. 
\bibitem{Bollen}J. Bollen and H. Mao. Twitter mood as a stock market predictor. IEEE Computer, 44(10):91–94.	
\bibitem{Pak}Alexander Pak and Patrick Paroubek. 2010. Twitter as a Corpus for Sentiment Analysis and Opinion Mining. In Proceedings of LREC. 
\bibitem{Mittal}Mittal, Anshul, and Arpit Goel. {\it Stock Prediction Using Twitter Sentiment Analysis}. Publication. N.p.: Stanford, n.d. Print.
\bibitem{SWN}SentiWordNet. An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining. http://sentiwordnet.isti.cnr.it/
\bibitem{Chen}Chen, Ray, and Marius Lazer. {\it Sentiment Analysis of Twitter Feeds for the Prediction of Stock Market Movement}. Publication. N.p.: Stanford, n.d. Print.
\bibitem{Baccianella} Baccianella, Stefano, Andrea Esuli, and Fabrizio Sebastiani. {\it SentiWordNet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining. Publication}. N.p.: Istituto Di Scienza E Tecnologie Dell'Informazione, 2010. Print.
\bibitem{Bifet}Bifet, Albert, and Eibe Frank. {\it Sentiment Knowledge Discovery in Twitter Streaming Data}. Publication. Hamilton: University of Waikato, n.d. Print.
\bibitem{Hurwitz}Hurwitz, E., and T. Marwala. {\it Common Mistakes When Applying Computational Intelligence and Machine Learning to Stock Market Modelling. \it} Publication. N.p.: Cornell, 2012. Print.
\bibitem{Brown}Brown, Eric D., "Will Twitter Make You a Better Investor? A Look at Sentiment, User Reputation and Their Effect on the Stock Market" (2012). {\it SAIS 2012 Proceedings}. Paper 7.
http://aisel.aisnet.org/sais2012/
\bibitem{Shangkun}Shangkun Deng, Takashi Mitsubuchi, Kei Shioda, Tatsuro Shimada, and Akito Sakurai. 2011. Combining Technical Analysis with Sentiment Analysis for Stock Price Prediction. In {\it Proceedings of the 2011 IEEE Ninth International Conference on Dependable, Autonomic and Secure Computing} (DASC '11). IEEE Computer Society, Washington, DC, USA, 800-807. DOI=10.1109/DASC.2011.138 http://dx.doi.org/10.1109/DASC.2011.138
\bibitem{Zhang}Zhang, DongSong, and Lina Zhou. "Discovering Golden Nuggets: Data Mining in Financial Application." Systems, Man, and Cybernetics, Part C: Applications and Reviews, IEEE Transactions on 34.4 (2004): 513-22. Web.

\end{thebibliography}

\end{document}             % End of document.
